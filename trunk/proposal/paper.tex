\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{psfig}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage[pdftex]{graphicx}
\usepackage{subfig}




% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\begin{document}

\title{Truthfulness Verification System}

\author{
Tathagata Dasgupta, ABM Musa\\
Email: \{tdasgu2, amusa2\}@uic.edu
}

\date{}
\maketitle


\section{Introduction}



\section{Current System}
Current system for truthfulness verification is called T-verifier, which uses two phase methods for truthfulness verification of statements. Each of these two phases rely heavily on search results returned by popular search engines. T-verifier takes the doubtful statements (DS) as input from the user along with the doubtful unit (DU). Phrase after removing DU from DS is called topic unit (TU).

At the first phase, T-verifier generates alternative statements by supplying TU to search engine and collecting the relevant alternate DUs. However, from basic web search may result in lot of alternate DUs that are not semantically or logically relevant to the original DS. Hence, T-verifier uses combination of seven features to rank alternate DUs. These features primarily exploits the facts that relevant alternative units frequently co-occur, people often mention both misconception and truthful conception together, data-type matching, and sense-closeness. T-verifier chose top 5 alternative statements based on top 5 alternate DUs obtained in this phase. 

At the second phase, top 5 alternative statements from phase 1 is supplied to the search engine again. Then the returned searched result is ranked by multiple rankers such as Alternative Unit Ranker, Hits ranker, Text-feature Ranker, Domain Authority Ranker etc. Then all those ranks are merged to form an overall ranking among the alternate statements and top statement in this final merged ranking is considered as truthful statement. 

%We are currently working to get the T-verifier code running on our
%systems, with inputs from Xian Li. So the descriptions below lacks some required
%details and makes some assumptions.
%
%
%As of now T-Verifier produces 5 alternative statements once it is provided with a
%doubtful statement (DS), a doubtful unit (DU), data type of the unit etc in the
%first phase. Though these sentences produced are quite accurate, the facility to
%pinpoint the correct alternative does exist yet. The project goal, may be formulated as:
%
%"Extract information from Wikipedia and ontologies ( such as Yago, DBpedia and
%Freebase) which helps to verify truthfulness of statements. Describe the
%extraction algorithm and how the information can be utilized to assist statement
%truthfulness verification."


%figure


\section {Propsed System}

\subsection{System Overview}



\subsection{Description of extraction algorithm}

For disambiguation among the alternative statements, Wikipedia is generally used
as an authoritative source. However, the contents of Wikipedia is not available
in the form that is consumable in a programmatic format. To address such
difficulties a number of projects Yago, DBpedia, Freebase have organized the
massive amount of data in a searchable fashion e.g DBpedia uses SPARQL endpoint,
Freebase uses MQL api. Open source implementation of Python wrappers exist for
both the interfaces exist and appear to be mature enough for our needs.

\subsection{Freebase} 
Freebase has information about approximately 20 million Topics,
each one having a unique Id, which can help distinguish multiple entities which
have similar names, such as Henry Ford the industrialist vs Henry Ford the
footballer. Most of the topics are associated with one or more types (such as
people, places, books, films, etc) and may have additional properties like "date
of birth" for a person or latitude and longitude for a location.


Python library: http://code.google.com/p/freebase-python/ The type system:
http://blog.freebase.com/2007/03/19/the-type-system-in-freebase/

Freebase seems to be the first point to start not only because of
the richness of the data available but also the available api. 




\subsection{Dbpedia}


http://wiki.dbpedia.org/Datasets?v=lbf
http://wiki.freebase.com/wiki/DBPedia
http://sourceforge.net/projects/sparql-wrapper/files/sparql-wrapper-python/1.4.2/ 


A few major tasks would be:

\subsection{Yago}
YAGO is a semantic knowledge base with over 900,000 entities (like persons,
organizations, cities, etc.). It knows about 6 million facts about these entities







\section{Truthfulness verification }
\subsection {Building queries from the data supplied by the user}

We take the content words from the topic unit($TU$), along with each of the
alternative units($AU$), and plugin the datatype($t$) to form a number of
queries.
\begin{align*}
q_{i} = \{{TU, AU_{i}, t_{AU_{i}}} \} \qquad \mbox {where $i = 1 \cdots n$} \\
\end{align*}
where $n$ is the number of alternative units generated by the first phase of
the existing system. By quering the various ontologies described above we will
get results which might have the following cases:

\begin{align*}
   freebase(q_{i}) &= \phi \\
    = r_{i}
\end{align*}

From what we have explored till now, by quering Freebase we can get the
various property, value pairs of the entity that we have supplied in the form
of $TU$. In case the result is $\phi$, it suggests that we do not have
information for this particulat combination $TU$ and $AU_{i}$. Due to the
richness of the information in Freebase we can assume with a considerable degree of confidence
that this combination is untrue. This is one way verification, but in case of
non-null results, how do we filter and rank the results with the available set
of information is something we can not formulate at this stage with out further
exploration.



\subsection{Application of Machine learning algorithms}: 
Though application of machine learning algorithms like using Naive Bayesian,
neural network 



\section{2.1 Supervised - Use a naive bayesian classifier}
\section{Unsupervised methods - clustering or Probabilistic LSI}
\section{Partially Supervised methods}



%\bibliography{mybib}
\bibliographystyle{alpha}

\end{document}

